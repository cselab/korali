#include "modules/neuralNetwork/layer/layer.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"
#include <gsl/gsl_blas.h>
#include <Eigen/Dense>

using namespace Eigen;

namespace korali
{
namespace neuralNetwork
{
std::vector<float> Layer::generateInitialHyperparameters()
{
  std::vector<float> hyperparameters;

  // If this is not the initial layer, calculate hyperparameters for weight and bias operation
  if (_prevLayer != NULL)
  {
    // Setting value for this layer's xavier constant
    float xavierConstant = sqrtf(6.0f) / sqrt(_nodeCount + _prevLayer->_nodeCount);

    // Adding layer's weights hyperparameter values
    for (size_t i = 0; i < _nodeCount; i++)
      for (size_t j = 0; j < _prevLayer->_nodeCount; j++)
        hyperparameters.push_back(xavierConstant * _nn->_xavierGenerator->getRandomNumber());

    // Adding layer's bias hyperparameter values
    for (size_t i = 0; i < _nodeCount; i++)
     hyperparameters.push_back(0.0f);
  }

  return hyperparameters;
}

void Layer::createHyperparameterMemory()
{
  // Checking Layer sizes
  if (_nodeCount == 0) KORALI_LOG_ERROR("Node count for layer (%lu) should be larger than zero.\n", _index);
  ssize_t OC = _nodeCount;

  // If this is not the input layer, we create the inner product (Wx + b) operation
  if (_prevLayer != NULL)
  {
    // Starting to count hyperparameters
    ssize_t IC = _prevLayer->_nodeCount;
    _hyperparameterCount = IC * OC;

    // Allocating weight memory
    _weightValues = (float*) malloc (IC * OC * sizeof(float));
    _weightDiff = (float*) malloc (IC * OC * sizeof(float));

      // Allocating bias memory
    _biasValues = (float*) malloc (OC * sizeof(float));
    _biasDiff = (float*) malloc (OC * sizeof(float));

    _hyperparameterCount += OC;
  }
}

void Layer::forwardActivationFunction()
{
 size_t N  = _nn->_batchSize;
 size_t OC = _nodeCount;

 if (_activationFunctionType == "Identity")
  memcpy(_inputValues, _outputValues, N*OC*sizeof(float));

 if (_activationFunctionType == "Elementwise/Linear")
  for (size_t i = 0; i < N*OC; i++)
   _outputValues[i] = _inputValues[i] * _activationFunctionAlpha;

 if (_activationFunctionType == "Elementwise/Log")
  for (size_t i = 0; i < N*OC; i++)
   _outputValues[i] = std::log(_inputValues[i]);

 if (_activationFunctionType == "Elementwise/ReLU")
  for (size_t i = 0; i < N*OC; i++)
   if (_inputValues[i] > 0.0f)
    _outputValues[i] = _inputValues[i];
   else
    _outputValues[i] = _inputValues[i] * _activationFunctionAlpha;

 if (_activationFunctionType == "Elementwise/Tanh")
  for (size_t i = 0; i < N*OC; i++)
   _outputValues[i] = std::tanh(_inputValues[i]);

 if (_activationFunctionType == "Elementwise/Logistic")
  for (size_t i = 0; i < N*OC; i++)
   _outputValues[i] = 1.0f / ( 1.0f + std::exp(-_inputValues[i]));

 if (_activationFunctionType == "Softmax")
 {
  for (size_t i = 0; i < N; i++)
  {
   float LSE = logSumExp(&_outputValues[i*OC], OC);
   for (size_t j = 0; j < OC; j++)
    _outputValues[i*OC + j] = std::exp(_inputValues[i*OC + j] - LSE);
  }
 }
}

void Layer::backwardActivationFunction()
{
 size_t N  = _nn->_batchSize;
 size_t OC = _nodeCount;

 if (_activationFunctionType == "Identity")
  memcpy(_inputDiff, _outputDiff, N*OC*sizeof(float));

 if (_activationFunctionType == "Elementwise/Linear")
  for (size_t i = 0; i < N*OC; i++)
   _inputDiff[i] = _outputDiff[i] * _activationFunctionAlpha;

 if (_activationFunctionType == "Elementwise/Log")
  for (size_t i = 0; i < N*OC; i++)
   _inputDiff[i] = _outputDiff[i] / _inputValues[i];

 if (_activationFunctionType == "Elementwise/ReLU")
  for (size_t i = 0; i < N*OC; i++)
   if (_inputValues[i] > 0.0f)
    _inputDiff[i] = _outputDiff[i];
   else
    _inputDiff[i] = _outputDiff[i] * _activationFunctionAlpha;

 if (_activationFunctionType == "Elementwise/Tanh")
  for (size_t i = 0; i < N*OC; i++)
   _inputDiff[i] = _outputDiff[i] * (1.0f - _outputValues[i]*_outputValues[i]);

 if (_activationFunctionType == "Elementwise/Logistic")
  for (size_t i = 0; i < N*OC; i++)
   _inputDiff[i] = _outputDiff[i] * _outputValues[i] * (1.0f - _outputValues[i]);

// if (_activationFunctionType == "Softmax")
// {
//  for (size_t i = 0; i < N; i++)
//  {
//   float LSE = logSumExp(&_outputValues[i*OC], OC);
//   for (size_t j = 0; j < OC; j++)
//    _outputValues[i*OC + j] = std::exp(_inputValues[i*OC + j] - LSE);
//  }
// }
}

void Layer::forwardWeightsAndBias()
{
 int N  = _nn->_batchSize;
 int IC = _prevLayer->_nodeCount;
 int OC = _nodeCount;

 // Setting the bias values to all minibatch inputs
 for (int i = 0; i < N; i++) memcpy(&_inputValues[i*OC], _biasValues, OC*sizeof(float));

 // Performing Wx + b computation
 gsl_matrix_float_view A = gsl_matrix_float_view_array(_prevLayer->_outputValues, N, IC);
 gsl_matrix_float_view B = gsl_matrix_float_view_array(_weightValues, OC, IC);
 gsl_matrix_float_view C = gsl_matrix_float_view_array(_inputValues, N, OC);

 int status = gsl_blas_sgemm(CblasNoTrans, CblasTrans, 1.0f, &A.matrix, &B.matrix, 1.0f, &C.matrix);
 if (status) KORALI_LOG_ERROR("GSL Error in forwardWeightsAndBias: %s\n", gsl_strerror(status));
}

void Layer::backwardData()
{
 int N  = _nn->_batchSize;
 int IC = _prevLayer->_nodeCount;
 int OC = _nodeCount;

 // Backward propagating Wx+b operation
 gsl_matrix_float_view A = gsl_matrix_float_view_array(_inputDiff, N, OC);
 gsl_matrix_float_view B = gsl_matrix_float_view_array(_weightValues, OC, IC);
 gsl_matrix_float_view C = gsl_matrix_float_view_array(_prevLayer->_outputDiff, N, IC);

 int status = gsl_blas_sgemm(CblasNoTrans, CblasNoTrans, 1.0f, &A.matrix, &B.matrix, 0.0f, &C.matrix);
 if (status) KORALI_LOG_ERROR("GSL Error in backwardData: %s\n", gsl_strerror(status));

// printf("Result: %f\n", _prevLayer->_outputDiff[0]);
}

void Layer::getWeightAndBiasGradients()
{
 int N  = _nn->_batchSize;
 int IC = _prevLayer->_nodeCount;
 int OC = _nodeCount;

 // Calculating weight gradients
 gsl_matrix_float_view A = gsl_matrix_float_view_array(_inputDiff, N, OC);
 gsl_matrix_float_view B = gsl_matrix_float_view_array(_prevLayer->_outputValues, N, IC);
 gsl_matrix_float_view C = gsl_matrix_float_view_array(_weightDiff, OC, IC);

 int status = gsl_blas_sgemm(CblasTrans, CblasNoTrans, 1.0f, &A.matrix, &B.matrix, 0.0f, &C.matrix);
 if (status) KORALI_LOG_ERROR("GSL Error in getWeightAndBiasGradients: %s\n", gsl_strerror(status));

 // Setting the bias values to all minibatch inputs
 memcpy(_biasDiff, &_inputDiff[0*OC], OC*sizeof(float));
 for (int i = 1; i < N; i++)
  for (int j = 0; j < OC; j++)
   _biasDiff[j] += _inputDiff[i*OC + j];
}


void Layer::createForwardPipeline()
{
  /****************************************************************************
   * Checking input/output configuration
   ****************************************************************************/

  // Obtaining batch size
  ssize_t N = _nn->_batchSize;

  // Checking Layer sizes
  if (_nodeCount == 0) KORALI_LOG_ERROR("Node count for layer (%lu) should be larger than zero.\n", _index);
  ssize_t OC = _nodeCount;

  /*********************************************************************************
  *  Initializing memory objects and primitives for FORWARD propagation
  *********************************************************************************/

  _inputValues = (float*) malloc (N * OC * sizeof(float));
  _outputValues = (float*) malloc (N * OC * sizeof(float));
  _inputDiff = (float*) malloc (N * OC * sizeof(float));
  _outputDiff = (float*) malloc (N * OC * sizeof(float));
}

} // namespace neuralNetwork

} // namespace korali
