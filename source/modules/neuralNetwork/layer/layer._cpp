#include "modules/neuralNetwork/layer/layer.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"
#include <Eigen/Dense>

using namespace Eigen;

namespace korali
{
namespace neuralNetwork
{
std::vector<float> Layer::generateInitialHyperparameters()
{
  std::vector<float> hyperparameters;

  // If this is not the initial layer, calculate hyperparameters for weight and bias operation
  if (_prevLayer != NULL)
  {
    // Setting value for this layer's xavier constant
    float xavierConstant = sqrtf(6.0f) / sqrt(_nodeCount + _prevLayer->_nodeCount);

    // Adding layer's weights hyperparameter values
    for (size_t i = 0; i < _nodeCount; i++)
      for (size_t j = 0; j < _prevLayer->_nodeCount; j++)
        hyperparameters.push_back(xavierConstant * _nn->_xavierGenerator->getRandomNumber());

    // Adding layer's bias hyperparameter values
    for (size_t i = 0; i < _nodeCount; i++)
     hyperparameters.push_back(0.0f);
  }

  return hyperparameters;
}

void Layer::createHyperparameterMemory()
{
  // Checking Layer sizes
  if (_nodeCount == 0) KORALI_LOG_ERROR("Node count for layer (%lu) should be larger than zero.\n", _index);
  ssize_t OC = _nodeCount;

  // If this is not the input layer, we create the inner product (Wx + b) operation
  if (_prevLayer != NULL)
  {
    // Starting to count hyperparameters
    ssize_t IC = _prevLayer->_nodeCount;
    _hyperparameterCount = IC * OC;

    // Allocating weight memory
    _weightValues = (float*) malloc (IC * OC * sizeof(float));
    _weightDiff = (float*) malloc (IC * OC * sizeof(float));

      // Allocating bias memory
    _biasValues = (float*) malloc (OC * sizeof(float));
    _biasDiff = (float*) malloc (OC * sizeof(float));

    _hyperparameterCount += OC;
  }
}

void Layer::forwardActivationFunction()
{
 size_t N  = _nn->_batchSize;
 size_t OC = _nodeCount;

 if (_activationFunctionType == "Identity")
  memcpy(_inputValues, _outputValues, N*OC*sizeof(float));

 if (_activationFunctionType == "Elementwise/Linear")
  for (size_t i = 0; i < N*OC; i++)
   _outputValues[i] = _inputValues[i] * _activationFunctionAlpha;

 if (_activationFunctionType == "Elementwise/Log")
  for (size_t i = 0; i < N*OC; i++)
   _outputValues[i] = std::log(_inputValues[i]);

 if (_activationFunctionType == "Elementwise/ReLU")
  for (size_t i = 0; i < N*OC; i++)
   if (_inputValues[i] > 0.0f)
    _outputValues[i] = _inputValues[i];
   else
    _outputValues[i] = _inputValues[i] * _activationFunctionAlpha;

 if (_activationFunctionType == "Elementwise/Tanh")
  for (size_t i = 0; i < N*OC; i++)
   _outputValues[i] = std::tanh(_inputValues[i]);

 if (_activationFunctionType == "Elementwise/Logistic")
  for (size_t i = 0; i < N*OC; i++)
   _outputValues[i] = 1.0f / ( 1.0f + std::exp(-_inputValues[i]));

 if (_activationFunctionType == "Softmax")
 {
  for (size_t i = 0; i < N; i++)
  {
   float LSE = logSumExp(&_inputValues[i*OC], OC);
   for (size_t j = 0; j < OC; j++)
    _outputValues[i*OC + j] = std::exp(_inputValues[i*OC + j] - LSE);
  }
 }
}

void Layer::backwardActivationFunction()
{
 size_t N  = _nn->_batchSize;
 size_t OC = _nodeCount;

 if (_activationFunctionType == "Identity")
  memcpy(_inputDiff, _outputDiff, N*OC*sizeof(float));

 if (_activationFunctionType == "Elementwise/Linear")
  for (size_t i = 0; i < N*OC; i++)
   _inputDiff[i] = _outputDiff[i] * _activationFunctionAlpha;

 if (_activationFunctionType == "Elementwise/Log")
  for (size_t i = 0; i < N*OC; i++)
   _inputDiff[i] = _outputDiff[i] / _inputValues[i];

 if (_activationFunctionType == "Elementwise/ReLU")
  for (size_t i = 0; i < N*OC; i++)
   if (_inputValues[i] > 0.0f)
    _inputDiff[i] = _outputDiff[i];
   else
    _inputDiff[i] = _outputDiff[i] * _activationFunctionAlpha;

 if (_activationFunctionType == "Elementwise/Tanh")
  for (size_t i = 0; i < N*OC; i++)
   _inputDiff[i] = _outputDiff[i] * (1.0f - _outputValues[i]*_outputValues[i]);

 if (_activationFunctionType == "Elementwise/Logistic")
  for (size_t i = 0; i < N*OC; i++)
   _inputDiff[i] = _outputDiff[i] * _outputValues[i] * (1.0f - _outputValues[i]);

 if (_activationFunctionType == "Softmax")
  for (size_t i = 0; i < N*OC; i++)
   _inputDiff[i] = _outputDiff[i] * _outputValues[i] * (1.0f - _outputValues[i]);
}

void Layer::forwardWeightsAndBias()
{
 int N  = _nn->_batchSize;
 int IC = _prevLayer->_nodeCount;
 int OC = _nodeCount;

 // Performing Wx computation
 Map<MatrixXf> matA(_weightValues, IC, OC);
 Map<MatrixXf> matB(_prevLayer->_outputValues, IC, N);
 Map<MatrixXf> matC(_inputValues, N, OC);

 matC = matA.transpose() * matB;

 // Adding Bias
 Map<VectorXf> biasValues(_biasValues, OC);
 for (int i = 0; i < N; i++)
  matC.row(i) += biasValues;
}

void Layer::backwardData()
{
 int N  = _nn->_batchSize;
 int IC = _prevLayer->_nodeCount;
 int OC = _nodeCount;

 // Backward propagating Wx+b operation
 Map<MatrixXf> matA(_weightValues, IC, OC);
 Map<MatrixXf> matB(_inputDiff, OC, N);
 Map<MatrixXf> matC(_prevLayer->_outputDiff, N, IC);

 matC = matA * matB;
}

void Layer::getWeightAndBiasGradients()
{
 int N  = _nn->_batchSize;
 int IC = _prevLayer->_nodeCount;
 int OC = _nodeCount;

 // Performing Weight gradient calculation
 Map<MatrixXf> matA(_prevLayer->_outputValues, IC, N);
 Map<MatrixXf> matB(_inputDiff, OC, N);
 Map<MatrixXf> matC(_weightDiff, OC, IC);

 matC = matA * matB.transpose();

 // Setting the bias values to all minibatch inputs

 Map<VectorXf> biasDiff(_biasDiff, OC);
 Map<MatrixXf> inputDiff(_inputDiff, N, OC);
 biasDiff = inputDiff.row(0);

 for (int i = 1; i < N; i++)
  biasDiff += inputDiff.row(i);
}


void Layer::createForwardPipeline()
{
  /****************************************************************************
   * Checking input/output configuration
   ****************************************************************************/

  // Obtaining batch size
  ssize_t N = _nn->_batchSize;

  // Checking Layer sizes
  if (_nodeCount == 0) KORALI_LOG_ERROR("Node count for layer (%lu) should be larger than zero.\n", _index);
  ssize_t OC = _nodeCount;

  /*********************************************************************************
  *  Initializing memory objects and primitives for FORWARD propagation
  *********************************************************************************/

  _inputValues = (float*) malloc (N * OC * sizeof(float));
  _outputValues = (float*) malloc (N * OC * sizeof(float));
  _inputDiff = (float*) malloc (N * OC * sizeof(float));
  _outputDiff = (float*) malloc (N * OC * sizeof(float));
}

} // namespace neuralNetwork

} // namespace korali
